---
phase: 03-language-model-with-streaming
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - requirements.txt
  - src/llm/__init__.py
  - src/llm/client.py
  - src/config.py
  - .env.example
  - tests/test_llm_client.py
autonomous: true
requirements: [CONV-01, CONV-02]

must_haves:
  truths:
    - "LLM client connects to any OpenAI-compatible API (vLLM, RunPod, OpenAI)"
    - "Client supports streaming token-by-token generation"
    - "Client is async-native (does not block FastAPI event loop)"
  artifacts:
    - path: "src/llm/client.py"
      provides: "Async OpenAI-compatible LLM client with streaming"
      exports: ["LLMClient"]
      min_lines: 50
    - path: "tests/test_llm_client.py"
      provides: "Unit tests for LLM client"
      min_lines: 30
  key_links:
    - from: "src/llm/client.py"
      to: "openai Python package"
      via: "AsyncOpenAI with custom base_url"
      pattern: "AsyncOpenAI"
    - from: "src/llm/client.py"
      to: "src/config.py"
      via: "LLM configuration settings"
      pattern: "settings"
---

<objective>
Create an async LLM client module using the OpenAI-compatible API for streaming text generation.

Purpose: Provide a reusable, configurable LLM client that works with vLLM on RunPod, local vLLM, or any OpenAI-compatible endpoint. Supports streaming token generation for low-latency responses.

Output: LLMClient class with async streaming chat completion, configurable via environment variables.
</objective>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-language-model-with-streaming/03-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install openai package and add LLM config</name>
  <files>requirements.txt, src/config.py, .env.example</files>
  <action>
Add the openai package to requirements.txt:

```
# Language Model (LLM) - OpenAI-compatible API client
openai>=2.0.0
```

Install:
```bash
pip install openai>=2.0.0
```

Update `src/config.py` to add LLM configuration:

```python
# Add to Settings class:
# LLM Configuration
llm_base_url: str = "http://localhost:8000/v1"  # vLLM endpoint (local or RunPod)
llm_api_key: str = "EMPTY"  # "EMPTY" for local vLLM, RunPod API key for cloud
llm_model: str = "google/gemma-3-27b-it"  # Model name on vLLM server
llm_max_tokens: int = 256  # Max response tokens
llm_temperature: float = 0.7  # Generation temperature
```

Update `.env.example` with LLM settings:
```
# LLM Configuration (vLLM / RunPod)
LLM_BASE_URL=http://localhost:8000/v1
LLM_API_KEY=EMPTY
LLM_MODEL=google/gemma-3-27b-it
LLM_MAX_TOKENS=256
LLM_TEMPERATURE=0.7
```
  </action>
  <verify>
```bash
pip list | grep openai
python -c "from openai import AsyncOpenAI; print('openai imported')"
python -c "from src.config import settings; print(f'LLM model: {settings.llm_model}')"
```
  </verify>
  <done>
openai package installed, LLM config added to settings and .env.example.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create LLMClient with async streaming</name>
  <files>src/llm/__init__.py, src/llm/client.py</files>
  <action>
Create `src/llm/client.py`:

```python
"""
LLM client for streaming text generation via OpenAI-compatible API.

Works with vLLM, RunPod, or any OpenAI-compatible endpoint.
Uses AsyncOpenAI for non-blocking streaming in FastAPI.
"""

import logging
from typing import AsyncGenerator, List, Dict, Optional
from openai import AsyncOpenAI
from src.config import settings

logger = logging.getLogger(__name__)


class LLMClient:
    """
    Async LLM client for streaming chat completions.

    Uses the OpenAI-compatible API to talk to vLLM, RunPod, or OpenAI.
    Streams tokens one-by-one for minimum latency.
    """

    def __init__(
        self,
        base_url: Optional[str] = None,
        api_key: Optional[str] = None,
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
    ):
        self.model = model or settings.llm_model
        self.max_tokens = max_tokens or settings.llm_max_tokens
        self.temperature = temperature or settings.llm_temperature

        self.client = AsyncOpenAI(
            api_key=api_key or settings.llm_api_key,
            base_url=base_url or settings.llm_base_url,
        )

        logger.info(f"LLMClient initialized: model={self.model}, base_url={base_url or settings.llm_base_url}")

    async def generate_streaming(
        self,
        messages: List[Dict[str, str]],
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
    ) -> AsyncGenerator[str, None]:
        """
        Generate a streaming response from the LLM.

        Args:
            messages: Chat messages in OpenAI format
                [{"role": "system", "content": "..."}, {"role": "user", "content": "..."}]
            max_tokens: Override default max tokens
            temperature: Override default temperature

        Yields:
            str: Individual text tokens as they're generated

        Example:
            async for token in client.generate_streaming(messages):
                print(token, end="", flush=True)
        """
        try:
            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=max_tokens or self.max_tokens,
                temperature=temperature or self.temperature,
                stream=True,
            )

            async for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content

        except Exception as e:
            logger.error(f"LLM generation error: {e}")
            raise

    async def generate(
        self,
        messages: List[Dict[str, str]],
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
    ) -> str:
        """
        Generate a complete (non-streaming) response.

        Args:
            messages: Chat messages in OpenAI format
            max_tokens: Override default max tokens
            temperature: Override default temperature

        Returns:
            str: Complete response text
        """
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=max_tokens or self.max_tokens,
                temperature=temperature or self.temperature,
            )
            return response.choices[0].message.content

        except Exception as e:
            logger.error(f"LLM generation error: {e}")
            raise
```

Create `src/llm/__init__.py`:
```python
from .client import LLMClient

__all__ = ["LLMClient"]
```

**Why AsyncOpenAI:** FastAPI runs on asyncio. Using sync OpenAI client would block the event loop and prevent handling concurrent audio chunks during LLM generation.

**Why configurable base_url:** Allows switching between local vLLM (development), RunPod (production), or even OpenAI (fallback) without code changes.
  </action>
  <verify>
```bash
python -c "from src.llm import LLMClient; print('LLMClient imported')"
python -c "from src.llm.client import LLMClient; c = LLMClient(); print('LLMClient instantiated')"
```
  </verify>
  <done>
LLMClient class exists with generate_streaming() and generate() methods. Module exports available via src.llm.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create unit tests for LLM client</name>
  <files>tests/test_llm_client.py</files>
  <action>
Create `tests/test_llm_client.py`:

```python
"""
Unit tests for LLM client.

Tests API contract and configuration â€” does NOT require a running vLLM server.
Uses mocking to verify client behavior without network calls.
"""

import pytest
from unittest.mock import AsyncMock, MagicMock, patch
from src.llm.client import LLMClient


def test_llm_client_initialization():
    """Test: LLMClient initializes with default settings"""
    client = LLMClient()
    assert client.model is not None
    assert client.max_tokens > 0
    assert 0 < client.temperature <= 2.0
    assert client.client is not None


def test_llm_client_custom_config():
    """Test: LLMClient accepts custom configuration"""
    client = LLMClient(
        base_url="http://custom:9000/v1",
        api_key="test-key",
        model="test-model",
        max_tokens=100,
        temperature=0.5,
    )
    assert client.model == "test-model"
    assert client.max_tokens == 100
    assert client.temperature == 0.5


@pytest.mark.asyncio
async def test_generate_streaming_yields_tokens():
    """Test: generate_streaming() yields string tokens"""
    client = LLMClient()

    # Mock the OpenAI streaming response
    mock_chunk_1 = MagicMock()
    mock_chunk_1.choices = [MagicMock()]
    mock_chunk_1.choices[0].delta.content = "Hello"

    mock_chunk_2 = MagicMock()
    mock_chunk_2.choices = [MagicMock()]
    mock_chunk_2.choices[0].delta.content = " world"

    mock_stream = AsyncMock()
    mock_stream.__aiter__ = MagicMock(return_value=iter([mock_chunk_1, mock_chunk_2]))

    with patch.object(client.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
        mock_create.return_value = mock_stream

        messages = [{"role": "user", "content": "Hi"}]
        tokens = []
        async for token in client.generate_streaming(messages):
            tokens.append(token)

        assert len(tokens) == 2
        assert tokens[0] == "Hello"
        assert tokens[1] == " world"


@pytest.mark.asyncio
async def test_generate_returns_complete_response():
    """Test: generate() returns complete string"""
    client = LLMClient()

    mock_response = MagicMock()
    mock_response.choices = [MagicMock()]
    mock_response.choices[0].message.content = "Hello world"

    with patch.object(client.client.chat.completions, 'create', new_callable=AsyncMock) as mock_create:
        mock_create.return_value = mock_response

        messages = [{"role": "user", "content": "Hi"}]
        result = await client.generate(messages)

        assert result == "Hello world"
        mock_create.assert_called_once()
```

**Why mocking:** Tests verify API contract and configuration without requiring a running vLLM server. E2E testing with real LLM happens in Plan 03.
  </action>
  <verify>
```bash
pytest tests/test_llm_client.py -v
```
All tests should pass without network calls.
  </verify>
  <done>
Unit tests pass, verifying LLMClient API contract with mocked responses.
  </done>
</task>

</tasks>

<verification>
1. **Dependencies installed:** `pip list | grep openai`
2. **Config updated:** `python -c "from src.config import settings; print(settings.llm_model)"`
3. **Module structure:** `python -c "from src.llm import LLMClient"`
4. **Tests pass:** `pytest tests/test_llm_client.py -v`
5. **No import errors or blocking issues.**
</verification>

<success_criteria>
- openai package installed
- LLMClient class with async streaming and non-streaming generation
- Configurable via environment variables (base_url, api_key, model)
- Unit tests pass with mocked responses
- Ready for conversation manager integration in Plan 02
</success_criteria>

<output>
After completion, create `.planning/phases/03-language-model-with-streaming/03-01-SUMMARY.md` following the summary template.
</output>
