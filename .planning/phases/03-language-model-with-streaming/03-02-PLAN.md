---
phase: 03-language-model-with-streaming
plan: 02
type: execute
wave: 1
depends_on: [03-01]
files_modified:
  - src/llm/conversation.py
  - tests/test_conversation.py
autonomous: true
requirements: [CONV-01, CONV-02]

must_haves:
  truths:
    - "Conversation manager tracks message history per call"
    - "System prompt defines AI personality and behavior"
    - "History is managed to stay within context window limits"
  artifacts:
    - path: "src/llm/conversation.py"
      provides: "Per-call conversation history management"
      exports: ["ConversationManager"]
      min_lines: 50
    - path: "tests/test_conversation.py"
      provides: "Unit tests for conversation management"
      min_lines: 30
  key_links:
    - from: "src/llm/conversation.py"
      to: "src/llm/client.py"
      via: "Formats messages for LLMClient"
      pattern: "messages"
---

<objective>
Create a conversation manager that tracks per-call message history and manages context for the LLM.

Purpose: Maintain conversation context across multiple turns within a single call, with a system prompt that defines the AI's conversational behavior.

Output: ConversationManager class that tracks user/assistant messages per call and formats them for the LLM client.
</objective>

<context>
@.planning/PROJECT.md
@.planning/phases/03-language-model-with-streaming/03-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ConversationManager with history tracking</name>
  <files>src/llm/conversation.py</files>
  <action>
Create `src/llm/conversation.py`:

```python
"""
Conversation manager for per-call message history.

Tracks user/assistant messages and manages context window for the LLM.
Each phone call gets its own ConversationManager instance.
"""

import logging
from typing import List, Dict, Optional

logger = logging.getLogger(__name__)

# Default system prompt for the AI phone assistant
DEFAULT_SYSTEM_PROMPT = (
    "You are a friendly and natural-sounding AI phone assistant. "
    "Keep your responses concise and conversational — you're on a phone call, "
    "not writing an essay. Respond in 1-3 sentences unless the caller asks for detail. "
    "Be warm, helpful, and speak naturally like a real person would on the phone."
)


class ConversationManager:
    """
    Per-call conversation history manager.

    Tracks the system prompt, user transcripts from STT, and
    assistant responses from the LLM. Formats messages for the
    OpenAI-compatible chat completion API.
    """

    def __init__(
        self,
        system_prompt: Optional[str] = None,
        max_history_messages: int = 20,
    ):
        """
        Initialize conversation manager for a single call.

        Args:
            system_prompt: System message defining AI behavior.
                Defaults to a natural phone assistant prompt.
            max_history_messages: Max user+assistant messages to keep.
                Oldest messages are trimmed when limit is reached.
                System prompt is always preserved.
        """
        self.system_prompt = system_prompt or DEFAULT_SYSTEM_PROMPT
        self.max_history_messages = max_history_messages
        self.history: List[Dict[str, str]] = []

    def add_user_message(self, text: str) -> None:
        """Add a user message (from STT final transcript)."""
        if not text or not text.strip():
            return
        self.history.append({"role": "user", "content": text.strip()})
        self._trim_history()
        logger.debug(f"Added user message: {text[:50]}...")

    def add_assistant_message(self, text: str) -> None:
        """Add an assistant message (from LLM response)."""
        if not text or not text.strip():
            return
        self.history.append({"role": "assistant", "content": text.strip()})
        self._trim_history()
        logger.debug(f"Added assistant message: {text[:50]}...")

    def get_messages(self) -> List[Dict[str, str]]:
        """
        Get full message list for LLM API call.

        Returns:
            List of messages with system prompt + conversation history.
            Format: [{"role": "system", "content": "..."}, {"role": "user", ...}, ...]
        """
        return [
            {"role": "system", "content": self.system_prompt},
            *self.history,
        ]

    def get_turn_count(self) -> int:
        """Get number of user turns in the conversation."""
        return sum(1 for m in self.history if m["role"] == "user")

    def _trim_history(self) -> None:
        """Trim oldest messages if history exceeds max."""
        while len(self.history) > self.max_history_messages:
            removed = self.history.pop(0)
            logger.debug(f"Trimmed oldest message: {removed['role']}")

    def reset(self) -> None:
        """Reset conversation history (keep system prompt)."""
        self.history = []
```

Update `src/llm/__init__.py` to export ConversationManager:
```python
from .client import LLMClient
from .conversation import ConversationManager

__all__ = ["LLMClient", "ConversationManager"]
```

**Why max 20 messages:** At ~100 tokens per turn, 20 messages ≈ 2000 tokens. With system prompt and generation, stays well within 8192 max_model_len. Covers 10+ back-and-forth exchanges (exceeds the 3+ turn requirement).

**Why trim oldest:** Simple and predictable. More sophisticated approaches (summarization, importance scoring) add complexity without clear benefit for phone calls.
  </action>
  <verify>
```bash
python -c "from src.llm import ConversationManager; cm = ConversationManager(); print('ConversationManager imported')"
python -c "
from src.llm import ConversationManager
cm = ConversationManager()
cm.add_user_message('Hello')
msgs = cm.get_messages()
print(f'Messages: {len(msgs)}, first role: {msgs[0][\"role\"]}')
"
```
  </verify>
  <done>
ConversationManager class exists with history tracking, trimming, and message formatting.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create unit tests for ConversationManager</name>
  <files>tests/test_conversation.py</files>
  <action>
Create `tests/test_conversation.py`:

```python
import pytest
from src.llm.conversation import ConversationManager, DEFAULT_SYSTEM_PROMPT


def test_initialization_with_default_prompt():
    """Test: ConversationManager initializes with default system prompt"""
    cm = ConversationManager()
    assert cm.system_prompt == DEFAULT_SYSTEM_PROMPT
    assert cm.history == []
    assert cm.max_history_messages == 20


def test_initialization_with_custom_prompt():
    """Test: ConversationManager accepts custom system prompt"""
    cm = ConversationManager(system_prompt="You are a test bot.")
    assert cm.system_prompt == "You are a test bot."


def test_add_user_message():
    """Test: add_user_message() adds to history"""
    cm = ConversationManager()
    cm.add_user_message("Hello there")
    assert len(cm.history) == 1
    assert cm.history[0] == {"role": "user", "content": "Hello there"}


def test_add_assistant_message():
    """Test: add_assistant_message() adds to history"""
    cm = ConversationManager()
    cm.add_assistant_message("Hi! How can I help?")
    assert len(cm.history) == 1
    assert cm.history[0] == {"role": "assistant", "content": "Hi! How can I help?"}


def test_empty_messages_ignored():
    """Test: Empty or whitespace-only messages are not added"""
    cm = ConversationManager()
    cm.add_user_message("")
    cm.add_user_message("   ")
    cm.add_assistant_message("")
    assert len(cm.history) == 0


def test_get_messages_includes_system_prompt():
    """Test: get_messages() prepends system prompt"""
    cm = ConversationManager()
    cm.add_user_message("Hello")
    messages = cm.get_messages()
    assert len(messages) == 2
    assert messages[0]["role"] == "system"
    assert messages[1]["role"] == "user"


def test_conversation_flow():
    """Test: Multi-turn conversation tracks correctly"""
    cm = ConversationManager()
    cm.add_user_message("Hello")
    cm.add_assistant_message("Hi there!")
    cm.add_user_message("How are you?")
    cm.add_assistant_message("I'm doing great!")
    cm.add_user_message("Good to hear")

    messages = cm.get_messages()
    assert len(messages) == 6  # system + 5 messages
    assert cm.get_turn_count() == 3


def test_history_trimming():
    """Test: History trims oldest messages when exceeding max"""
    cm = ConversationManager(max_history_messages=4)
    cm.add_user_message("msg1")
    cm.add_assistant_message("resp1")
    cm.add_user_message("msg2")
    cm.add_assistant_message("resp2")
    cm.add_user_message("msg3")  # This should trim msg1

    assert len(cm.history) == 4
    assert cm.history[0]["content"] == "resp1"  # msg1 was trimmed


def test_reset_clears_history():
    """Test: reset() clears history but keeps system prompt"""
    cm = ConversationManager()
    cm.add_user_message("Hello")
    cm.add_assistant_message("Hi!")
    cm.reset()

    assert len(cm.history) == 0
    assert cm.system_prompt is not None
    messages = cm.get_messages()
    assert len(messages) == 1  # Just system prompt
```
  </action>
  <verify>
```bash
pytest tests/test_conversation.py -v
```
All tests should pass without any external dependencies.
  </verify>
  <done>
Unit tests pass, verifying conversation history tracking, trimming, and message formatting.
  </done>
</task>

</tasks>

<verification>
1. **Module imports:** `python -c "from src.llm import ConversationManager"`
2. **Tests pass:** `pytest tests/test_conversation.py -v`
3. **History tracking works:** Multi-turn conversation tracked correctly
4. **Trimming works:** Oldest messages removed when max exceeded
</verification>

<success_criteria>
- ConversationManager tracks per-call message history
- System prompt prepended to all message lists
- History trimming keeps context within limits
- Unit tests pass
- Ready for handler integration in Plan 03
</success_criteria>

<output>
After completion, create `.planning/phases/03-language-model-with-streaming/03-02-SUMMARY.md` following the summary template.
</output>
