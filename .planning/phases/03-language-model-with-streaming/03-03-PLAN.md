---
phase: 03-language-model-with-streaming
plan: 03
type: execute
wave: 2
depends_on: [03-01, 03-02]
files_modified:
  - src/twilio/handlers.py
  - tests/test_e2e_llm.md
autonomous: false
requirements: [CONV-01, CONV-02]

must_haves:
  truths:
    - "Turn complete triggers LLM response generation"
    - "LLM response is streamed token-by-token and logged"
    - "Conversation history persists across turns within a call"
    - "System handles LLM errors gracefully without crashing"
  artifacts:
    - path: "src/twilio/handlers.py"
      provides: "Integrated STT + VAD + LLM pipeline"
      contains: ["LLMClient", "ConversationManager"]
      min_lines: 180
    - path: "tests/test_e2e_llm.md"
      provides: "E2E testing checklist for LLM integration"
      min_lines: 40
  key_links:
    - from: "src/twilio/handlers.py"
      to: "src/llm/client.py"
      via: "LLMClient for streaming generation"
      pattern: "LLMClient"
    - from: "src/twilio/handlers.py"
      to: "src/llm/conversation.py"
      via: "ConversationManager per call"
      pattern: "ConversationManager"
---

<objective>
Integrate LLM client and conversation manager into Twilio handlers, replacing transcript-only logging with streaming AI responses.

Purpose: When a user finishes speaking (turn complete), the system generates a conversational response via the LLM and logs it. This completes the Phase 3 goal of "Natural conversational responses with streaming generation."

Output: Working STT → LLM pipeline that transcribes speech, generates AI responses, and maintains conversation history per call.
</objective>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-language-model-with-streaming/03-RESEARCH.md
@src/twilio/handlers.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integrate LLM into handlers pipeline</name>
  <files>src/twilio/handlers.py</files>
  <action>
Update `src/twilio/handlers.py` to add LLM response generation on turn complete:

**1. Add imports:**
```python
from src.llm.client import LLMClient
from src.llm.conversation import ConversationManager
```

**2. Update ConnectionManager:**
```python
class ConnectionManager:
    def __init__(self):
        # ... existing fields ...

        # LLM client (shared, stateless)
        self.llm_client = None

        # Conversation managers per call (per-call state)
        self.conversations: Dict[str, ConversationManager] = {}

    def get_llm_client(self) -> LLMClient:
        """Get or create shared LLM client"""
        if self.llm_client is None:
            self.llm_client = LLMClient()
        return self.llm_client

    def get_conversation(self, stream_sid: str) -> ConversationManager:
        """Get or create conversation manager for this call"""
        if stream_sid not in self.conversations:
            self.conversations[stream_sid] = ConversationManager()
        return self.conversations[stream_sid]
```

**3. Update handle_media() turn complete section:**

Replace the current turn complete handling:
```python
# Current (Phase 2):
if vad_result["turn_complete"]:
    logger.info(f"[{stream_sid}] Turn complete after {vad_result['silence_duration_ms']}ms silence")
    final = await asyncio.to_thread(stt_processor.finalize_turn)
    logger.info(f"[{stream_sid}] Final: {final['text']}")
    logger.info(f"[{stream_sid}] User said: {final['text']}")
    vad_detector.reset()
```

With:
```python
if vad_result["turn_complete"]:
    logger.info(f"[{stream_sid}] Turn complete after {vad_result['silence_duration_ms']}ms silence")

    # Finalize STT transcript
    final = await asyncio.to_thread(stt_processor.finalize_turn)
    user_text = final["text"]
    logger.info(f"[{stream_sid}] User said: {user_text}")

    if user_text and user_text.strip():
        # Add user message to conversation history
        conversation = manager.get_conversation(stream_sid)
        conversation.add_user_message(user_text)

        # Generate LLM response (streaming)
        llm_client = manager.get_llm_client()
        messages = conversation.get_messages()

        response_tokens = []
        try:
            async for token in llm_client.generate_streaming(messages):
                response_tokens.append(token)

            response_text = "".join(response_tokens)
            logger.info(f"[{stream_sid}] AI response: {response_text}")

            # Add assistant response to conversation history
            conversation.add_assistant_message(response_text)

            # TODO Phase 4: Send response_text to TTS for audio playback
            logger.info(f"[{stream_sid}] Turn {conversation.get_turn_count()}: User='{user_text[:50]}' AI='{response_text[:50]}'")

        except Exception as e:
            logger.error(f"[{stream_sid}] LLM error: {e}")
            # Don't crash the call on LLM errors

    # Reset VAD for next turn
    vad_detector.reset()
```

**4. Update handle_stop() to cleanup conversation:**
```python
# Add to handle_stop():
if stream_sid and stream_sid in manager.conversations:
    del manager.conversations[stream_sid]
```

**Critical notes:**
- LLM client is shared (stateless connection pool), conversation managers are per-call
- Streaming collects all tokens then logs the full response
- Future Phase 4 will pipe tokens to TTS as they arrive
- LLM errors are caught and logged, don't crash the call
- Empty transcripts (silence only) skip LLM generation
  </action>
  <verify>
```bash
python -c "from src.twilio.handlers import ConnectionManager; cm = ConnectionManager(); print('Handlers import OK')"
python -c "
from src.twilio.handlers import ConnectionManager
cm = ConnectionManager()
llm = cm.get_llm_client()
conv = cm.get_conversation('test-stream')
print(f'LLM client: {llm is not None}, Conversation: {conv is not None}')
"
```
  </verify>
  <done>
handle_media() generates LLM responses on turn complete, with conversation history tracking per call.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create E2E testing checklist for LLM integration</name>
  <files>tests/test_e2e_llm.md</files>
  <action>
Create `tests/test_e2e_llm.md`:

```markdown
# Phase 3: Language Model E2E Testing

**Goal:** Verify LLM generates contextual responses to user speech during phone calls.

**Prerequisites:**
- Phase 2 STT/VAD working
- vLLM server running with Gemma 3 27B (RunPod or local)
- LLM_BASE_URL and LLM_API_KEY configured in .env
- Server logs accessible

## Test 1: Basic Response Generation

**Steps:**
1. Start server: `uvicorn src.main:app --log-level=info`
2. Call Twilio number
3. Speak: "Hello, how are you?"
4. Wait for turn complete + LLM response in logs

**Pass Criteria:**
- [ ] Turn complete triggers LLM generation
- [ ] AI response logged (contextually relevant greeting)
- [ ] Response generated within 2 seconds of turn complete
- [ ] No crashes or errors

## Test 2: Multi-Turn Conversation (3+ turns)

**Steps:**
1. Call Twilio number
2. Turn 1: "Hello" → wait for AI response log
3. Turn 2: "What can you help me with?" → wait for AI response log
4. Turn 3: "Tell me a joke" → wait for AI response log

**Pass Criteria:**
- [ ] All 3 turns generate separate responses
- [ ] Responses are contextually relevant
- [ ] Conversation history maintained (AI knows previous context)
- [ ] Turn count logged correctly

## Test 3: Context Retention

**Steps:**
1. Call Twilio number
2. Say: "My name is Alex"
3. Wait for response
4. Say: "What's my name?"

**Pass Criteria:**
- [ ] AI remembers the name from previous turn
- [ ] Response references "Alex"

## Test 4: LLM Error Handling

**Steps:**
1. Set LLM_BASE_URL to invalid endpoint in .env
2. Start server and make a call
3. Speak and trigger turn complete

**Pass Criteria:**
- [ ] Error logged but call doesn't crash
- [ ] WebSocket stays connected
- [ ] Subsequent turns still attempt LLM generation

## Known Limitations (Phase 3)
- AI responds in text logs only (no audio playback — Phase 4: TTS)
- No interruption handling (Phase 5)
- Latency depends on vLLM/RunPod configuration
```
  </action>
  <verify>
```bash
ls tests/test_e2e_llm.md && echo "E2E checklist created"
```
  </verify>
  <done>
E2E testing checklist created with 4 test scenarios.
  </done>
</task>

<task type="checkpoint:human-verify" gate="non-blocking">
  <what-built>
Complete Phase 3 LLM pipeline:
- Async LLM client with streaming via OpenAI-compatible API
- Per-call conversation history management
- Integration into Twilio handlers (STT → LLM)
- Turn complete triggers AI response generation
  </what-built>

  <how-to-verify>
**To test with a real vLLM server:**

1. Deploy vLLM on RunPod or run locally
2. Set LLM_BASE_URL and LLM_API_KEY in .env
3. Start server: `uvicorn src.main:app --log-level=info`
4. Call Twilio number, speak, and check logs for AI responses

**Note:** This requires a running vLLM server with Gemma 3 27B. If not available yet, the system will log LLM errors but won't crash. Full deployment is Phase 6.
  </how-to-verify>

  <resume-signal>
Type "approved" if LLM integration works, or note issues for debugging.
This checkpoint is non-blocking — Phase 4 can proceed without live LLM testing.
  </resume-signal>
</task>

</tasks>

<verification>
1. **Integration complete:**
   ```bash
   grep -q "LLMClient" src/twilio/handlers.py
   grep -q "ConversationManager" src/twilio/handlers.py
   grep -q "generate_streaming" src/twilio/handlers.py
   ```
2. **E2E checklist exists:** `ls tests/test_e2e_llm.md`
3. **All tests pass:** `pytest tests/ -v`
</verification>

<success_criteria>
- Turn complete triggers LLM streaming response generation
- Conversation history tracked per call (3+ turns)
- LLM errors handled gracefully
- E2E testing checklist created
- Ready for Phase 4 (TTS — pipe LLM response tokens to speech synthesis)
</success_criteria>

<output>
After completion, create `.planning/phases/03-language-model-with-streaming/03-03-SUMMARY.md` following the summary template.
</output>
