---
phase: 02-speech-to-text-with-streaming
plan: 03
type: execute
wave: 2
depends_on: [02-01, 02-02]
files_modified:
  - src/twilio/handlers.py
  - src/main.py
  - README.md
  - tests/test_e2e_stt.md
autonomous: false
requirements: [STT-01, STT-02, STT-03]

must_haves:
  truths:
    - "User speaks during call and sees real-time transcript logged"
    - "System detects when user stops speaking within 300ms"
    - "System produces accurate transcription within 200ms of speech ending"
    - "System handles natural pauses without premature cutoff"
  artifacts:
    - path: "src/twilio/handlers.py"
      provides: "Integrated STT + VAD processing in handle_media"
      contains: "STTProcessor"
      min_lines: 150
    - path: "tests/test_e2e_stt.md"
      provides: "End-to-end testing checklist for STT"
      min_lines: 60
  key_links:
    - from: "src/twilio/handlers.py"
      to: "src/stt/processor.py"
      via: "STTProcessor instance per call"
      pattern: "STTProcessor\\(\\)"
    - from: "src/twilio/handlers.py"
      to: "src/vad/detector.py"
      via: "VADDetector instance per call"
      pattern: "VADDetector\\(\\)"
    - from: "src/twilio/handlers.py"
      to: "src/audio/conversion.py"
      via: "mulaw_to_pcm conversion before STT"
      pattern: "mulaw_to_pcm"
    - from: "src/twilio/handlers.py"
      to: "src/audio/resampling.py"
      via: "resample_8k_to_16k before STT/VAD"
      pattern: "resample_8k_to_16k"
---

<objective>
Integrate STT and VAD modules into Twilio handlers, replacing echo with real-time speech transcription.

Purpose: Enable live speech-to-text during phone calls with fast turn detection, completing the Phase 2 goal of "Real-time speech transcription with fast turn detection."

Output: Working STT pipeline that transcribes caller speech in real-time, detects turn completion within 300ms, and logs transcripts for verification.
</objective>

<execution_context>
@/Users/lucassenechal/.claude/get-shit-done/workflows/execute-plan.md
@/Users/lucassenechal/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-speech-to-text-with-streaming/02-RESEARCH.md

# Phase 1 summaries (existing handlers structure)
@.planning/phases/01-telephony-foundation-audio-pipeline/01-01-SUMMARY.md
@.planning/phases/01-telephony-foundation-audio-pipeline/01-02-SUMMARY.md
@.planning/phases/01-telephony-foundation-audio-pipeline/01-03-SUMMARY.md

# Phase 2 plan summaries (once 01 and 02 complete)
# Will be available when execute-plan runs this plan
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integrate STT and VAD into ConnectionManager and handlers</name>
  <files>src/twilio/handlers.py</files>
  <action>
Update `src/twilio/handlers.py` to integrate STT and VAD processing:

**Changes required:**

1. **Add imports:**
```python
from src.stt import STTProcessor
from src.vad import VADDetector
from src.audio.conversion import mulaw_to_pcm
from src.audio.resampling import resample_8k_to_16k
import base64
import asyncio
```

2. **Update ConnectionManager to track STT/VAD instances:**
```python
class ConnectionManager:
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}
        self.streamers: Dict[str, AudioStreamer] = {}
        self.state_managers: Dict[str, CallStateManager] = {}

        # NEW: STT and VAD processors (shared across calls for model reuse)
        self.stt_processor = None  # Initialized once on first call

        # NEW: VAD instances per call (need separate state per caller)
        self.vad_detectors: Dict[str, VADDetector] = {}

    def get_stt_processor(self):
        """Get or create shared STT processor"""
        if self.stt_processor is None:
            self.stt_processor = STTProcessor(
                model_size="distil-large-v3",
                language="en"
            )
        return self.stt_processor

    def get_vad_detector(self, stream_sid: str) -> VADDetector:
        """Get or create VAD detector for this call"""
        if stream_sid not in self.vad_detectors:
            self.vad_detectors[stream_sid] = VADDetector(
                threshold=0.5,
                min_silence_ms=550,
                min_speech_ms=250
            )
        return self.vad_detectors[stream_sid]
```

3. **Replace echo in handle_media() with STT processing:**

**Current code (from Phase 1, line ~90):**
```python
async def handle_media(connection_manager, stream_sid, payload):
    # TODO: Process audio (convert mu-law → PCM, send to STT)
    # Current: Echo audio back
    await connection_manager.queue_audio(stream_sid, payload)
```

**New code:**
```python
async def handle_media(connection_manager, stream_sid, payload):
    """
    Process incoming audio through STT + VAD pipeline.

    Flow:
    1. Decode base64 mu-law from Twilio
    2. Convert mu-law → PCM 8kHz
    3. Resample 8kHz → 16kHz for STT/VAD
    4. Run VAD to detect speech/silence
    5. Feed audio to STT if speech detected
    6. Finalize transcript on turn complete
    """
    # Decode Twilio audio
    audio_mulaw = base64.b64decode(payload)

    # Phase 1 conversion pipeline
    pcm_8khz = mulaw_to_pcm(audio_mulaw)
    pcm_16khz = resample_8k_to_16k(pcm_8khz)

    # Get processors
    stt_processor = connection_manager.get_stt_processor()
    vad_detector = connection_manager.get_vad_detector(stream_sid)

    # Run VAD on chunk
    vad_result = vad_detector.process_chunk(pcm_16khz)

    if vad_result["is_speech"]:
        # Speech detected - feed to STT
        # Use asyncio.to_thread to avoid blocking event loop (per 02-RESEARCH.md Anti-Pattern 5)
        async for transcript in asyncio.to_thread(
            lambda: list(stt_processor.process_audio_chunk(pcm_16khz))
        ):
            for partial in transcript:
                if partial["type"] == "partial":
                    logger.info(f"[{stream_sid}] Partial: {partial['text']}")

    # Check for turn complete
    if vad_result["turn_complete"]:
        logger.info(f"[{stream_sid}] Turn complete after {vad_result['silence_duration_ms']}ms silence")

        # Finalize transcript
        final = await asyncio.to_thread(stt_processor.finalize_turn)
        logger.info(f"[{stream_sid}] Final: {final['text']}")

        # Update call state
        state_manager = connection_manager.state_managers.get(stream_sid)
        if state_manager:
            # Store transcript (future: send to LLM in Phase 3)
            logger.info(f"[{stream_sid}] User said: {final['text']}")

        # Reset VAD for next turn
        vad_detector.reset()
```

4. **Update handle_stop() to cleanup VAD:**
```python
async def handle_stop(connection_manager, stream_sid):
    """Handle call end event"""
    # ... existing cleanup ...

    # NEW: Remove VAD detector
    if stream_sid in connection_manager.vad_detectors:
        del connection_manager.vad_detectors[stream_sid]
```

**Critical implementation notes:**

- **Use asyncio.to_thread():** Whisper is synchronous. Per 02-RESEARCH.md Anti-Pattern 5, must wrap in async to avoid blocking FastAPI event loop.

- **Shared STT processor, per-call VAD:** STT model loads once (expensive), VAD has per-call state (cheap to instantiate).

- **Log transcripts, don't send back yet:** Phase 2 focuses on transcription. Phase 3 will add LLM responses. Phase 4 will add TTS playback.

- **Respect Phase 1 conversion functions:** Use existing `mulaw_to_pcm` and `resample_8k_to_16k` from Phase 1. Don't duplicate conversion logic.
  </action>
  <verify>
```bash
# Check imports work
python -c "from src.twilio.handlers import ConnectionManager; cm = ConnectionManager(); print('Imports OK')"

# Check STT processor initializes
python -c "from src.twilio.handlers import ConnectionManager; cm = ConnectionManager(); stt = cm.get_stt_processor(); print('STT initialized')"

# Check VAD detector initializes
python -c "from src.twilio.handlers import ConnectionManager; cm = ConnectionManager(); vad = cm.get_vad_detector('test'); print('VAD initialized')"
```
All should succeed. STT initialization takes 10-30 seconds on first run.
  </verify>
  <done>
handle_media() processes audio through STT + VAD pipeline, logs partial and final transcripts, detects turn completion.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update README and create E2E testing checklist</name>
  <files>README.md, tests/test_e2e_stt.md</files>
  <action>
Update `README.md` with Phase 2 changes:

**Add to "Features" section:**
```markdown
## Features

### Phase 1: Telephony Foundation ✓
- Twilio WebSocket integration for bidirectional audio streaming
- Audio format conversion (mu-law ↔ PCM)
- Audio resampling (8kHz ↔ 16kHz)
- Call state management

### Phase 2: Speech-to-Text with Streaming ✓
- Real-time speech transcription using faster-whisper + whisper_streaming
- Voice activity detection (VAD) using Silero VAD
- Turn detection with <300ms latency
- Streaming partial transcripts during speech
```

**Add to "Installation" section:**
```markdown
### Phase 2 Dependencies (STT + VAD)
```bash
pip install faster-whisper>=1.2.0
pip install git+https://github.com/ufal/whisper_streaming
pip install silero-vad>=5.1
```
```

**Add to "Testing" section:**
```markdown
### Phase 2: STT Testing
See `tests/test_e2e_stt.md` for end-to-end speech transcription testing.

Expected behavior:
- Call Twilio number
- Speak into phone
- Check server logs for partial transcripts (real-time)
- Stop speaking (pause 1-2 seconds)
- Check server logs for final transcript (turn complete)
```

Create `tests/test_e2e_stt.md`:

```markdown
# Phase 2: Speech-to-Text End-to-End Testing

**Goal:** Verify real-time speech transcription with turn detection on live phone calls.

## Prerequisites
- Phase 1 telephony infrastructure operational (Railway deployment or ngrok)
- Phase 2 STT/VAD modules installed
- Twilio phone number configured
- Server logs accessible

## Test 1: Basic Speech Transcription

**Steps:**
1. Start server with logging enabled: `uvicorn src.main:app --log-level=info`
2. Call Twilio number from phone
3. Wait for connection (should hear silence, no echo)
4. Speak clearly: "Hello, this is a test"
5. Pause for 2 seconds (silence)

**Pass Criteria:**
- [ ] Server logs show "Partial: Hello" (within 1 second of speaking)
- [ ] Server logs show "Partial: Hello, this is a test" (real-time updates)
- [ ] Server logs show "Turn complete" after 550-700ms silence
- [ ] Server logs show "Final: Hello, this is a test" with >80% accuracy
- [ ] No crashes or errors

**Fail Signals:**
- Transcripts are gibberish or empty
- Turn complete never triggers
- Turn complete triggers mid-sentence (premature cutoff)
- Server crashes with ImportError or model loading error

## Test 2: Turn Detection Timing

**Steps:**
1. Call Twilio number
2. Speak: "Testing turn detection"
3. Stop speaking and measure time until turn complete log

**Pass Criteria:**
- [ ] Turn complete triggers within 300-700ms of silence
- [ ] VAD doesn't trigger during mid-sentence pauses
- [ ] Turn complete logged with silence duration

**Fail Signals:**
- Turn complete takes >1 second (too slow)
- Turn complete triggers during natural pauses (false positive)

## Test 3: Natural Pauses (No Premature Cutoff)

**Steps:**
1. Call Twilio number
2. Speak with deliberate pauses: "I need to... um... schedule an appointment"
3. Check if turn completes prematurely

**Pass Criteria:**
- [ ] Turn complete only triggers after final word "appointment"
- [ ] Partial transcripts include "um" and pauses
- [ ] Final transcript is complete sentence

**Fail Signals:**
- Turn complete triggers after "to..." or "um..."
- Transcript cuts off mid-sentence

## Test 4: Accuracy on PSTN Audio Quality

**Steps:**
1. Call from different phone types (cell phone, landline if available)
2. Speak clearly with no background noise
3. Test phrases: "Can you hear me clearly", "The quick brown fox jumps"

**Pass Criteria:**
- [ ] Transcripts >80% accurate on clear speech
- [ ] Common words transcribed correctly ("the", "can", "you")
- [ ] Proper nouns may be inaccurate (acceptable for Phase 2)

**Fail Signals:**
- Transcripts <50% accurate on clear speech
- Simple words consistently wrong

## Test 5: Multiple Turns in One Call

**Steps:**
1. Call Twilio number
2. Speak first sentence: "Hello"
3. Wait for turn complete
4. Speak second sentence: "How are you"
5. Wait for turn complete
6. Hang up

**Pass Criteria:**
- [ ] Both turns detected separately
- [ ] Final transcripts for both turns logged
- [ ] VAD resets between turns (no context bleed)

**Fail Signals:**
- Second turn doesn't trigger turn complete
- Transcripts merge across turns

## Test 6: Background Noise Handling

**Steps:**
1. Call from noisy environment (traffic, TV, music)
2. Speak clearly: "This is a test"
3. Check transcripts and VAD behavior

**Pass Criteria:**
- [ ] VAD doesn't trigger on background noise alone
- [ ] Speech transcripts reasonably accurate despite noise
- [ ] Turn detection still works

**Fail Signals:**
- VAD triggers constantly on background noise
- Transcripts are nonsense from background sounds
- Turn detection fails in noisy environment

**Note:** If Test 6 fails, may need to increase VAD threshold from 0.5 to 0.6-0.7 in src/twilio/handlers.py per 02-RESEARCH.md recommendations.

## Expected Logs

**Successful call logs:**
```
INFO: [abc123] Partial: Hello
INFO: [abc123] Partial: Hello, this
INFO: [abc123] Partial: Hello, this is a test
INFO: [abc123] Turn complete after 620ms silence
INFO: [abc123] Final: Hello, this is a test
INFO: [abc123] User said: Hello, this is a test
```

## Known Limitations (Phase 2)

- System only transcribes, doesn't respond yet (Phase 3: LLM)
- No audio playback to caller (Phase 4: TTS)
- No interruption handling (Phase 5)
- VAD thresholds are fixed (may need tuning based on testing)

## Troubleshooting

**No transcripts appearing:**
- Check model downloaded: `ls ~/.cache/huggingface/hub/` should show distil-whisper model
- Check imports: `python -c "from src.stt import STTProcessor"`

**Turn complete never triggers:**
- VAD may be too sensitive. Try increasing min_silence_ms to 800-1000ms
- Check audio reaching VAD: Add logging in handle_media()

**Premature turn complete (cuts off mid-sentence):**
- VAD min_silence_ms too low. Increase from 550ms to 700-800ms
- Check for PSTN audio quality issues

**Poor transcription accuracy:**
- PSTN audio is 8kHz mu-law (low quality). 70-80% accuracy is normal.
- Try speaking more clearly and slowly
- Check if background noise is interfering
```

  </action>
  <verify>
```bash
# Check README updated
grep -q "Phase 2: Speech-to-Text" README.md && echo "README updated"

# Check E2E test checklist exists
ls tests/test_e2e_stt.md && echo "E2E checklist created"

# Check checklist has min 60 lines
wc -l tests/test_e2e_stt.md | awk '{if ($1 >= 60) print "Checklist has " $1 " lines (min 60)"}'
```
  </verify>
  <done>
README updated with Phase 2 features and installation. E2E testing checklist created with 6 test scenarios and clear pass/fail criteria.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete Phase 2 speech-to-text pipeline:
- Streaming STT using faster-whisper + whisper_streaming
- VAD turn detection using Silero VAD
- Integration into Twilio handlers replacing echo
- Real-time transcription logged during calls
  </what-built>

  <how-to-verify>
**Test the STT pipeline with a real phone call:**

1. **Start server with logging:**
   ```bash
   cd /Users/lucassenechal/Projects/Client\ Caller
   uvicorn src.main:app --log-level=info
   ```

   Or if using Railway deployment:
   ```bash
   # Check Railway logs in dashboard
   ```

2. **Call your Twilio number** from any phone

3. **Speak clearly:** "Hello, this is a test of speech recognition"

4. **Watch server logs in real-time** — you should see:
   ```
   INFO: Partial: Hello
   INFO: Partial: Hello, this is a test
   INFO: Partial: Hello, this is a test of speech recognition
   ```

5. **Stop speaking and wait 1-2 seconds** — you should see:
   ```
   INFO: Turn complete after 620ms silence
   INFO: Final: Hello, this is a test of speech recognition
   INFO: User said: Hello, this is a test of speech recognition
   ```

6. **Test multiple turns:**
   - Speak: "First sentence"
   - Wait for turn complete log
   - Speak: "Second sentence"
   - Wait for turn complete log
   - Both should appear as separate final transcripts

7. **Test turn detection timing:**
   - Speak with natural pauses: "I need to... um... test this"
   - Turn complete should only trigger AFTER the full sentence, not during "um" pause

**Use tests/test_e2e_stt.md for complete testing checklist (6 scenarios)**

**Expected outcomes:**
✓ Partial transcripts appear in real-time as you speak
✓ Turn complete triggers 300-700ms after you stop speaking
✓ Final transcript is >70% accurate (PSTN audio quality)
✓ Multiple turns detected separately
✓ No premature cutoff during natural pauses
✓ No crashes or errors

**Common issues:**
- If no transcripts appear: Check model downloaded, check imports
- If turn complete never triggers: Increase min_silence_ms to 800ms
- If premature cutoff: Increase min_silence_ms to 700-800ms
- If poor accuracy: PSTN quality is 8kHz (expected), try speaking more clearly
  </how-to-verify>

  <resume-signal>
Type "approved" if STT pipeline works as expected (transcripts appear, turn detection works, accuracy >70%).

If issues found, describe what's not working and I'll help debug.
  </resume-signal>
</task>

</tasks>

<verification>
All verification criteria (after checkpoint approval):

1. **Integration complete:**
   ```bash
   grep -q "STTProcessor" src/twilio/handlers.py
   grep -q "VADDetector" src/twilio/handlers.py
   grep -q "mulaw_to_pcm" src/twilio/handlers.py
   grep -q "resample_8k_to_16k" src/twilio/handlers.py
   ```

2. **Documentation updated:**
   ```bash
   grep -q "Phase 2" README.md
   ls tests/test_e2e_stt.md
   ```

3. **End-to-end STT verified:**
   - User completed phone call testing
   - Transcripts appear in logs
   - Turn detection works
   - Accuracy acceptable for PSTN quality

4. **No blocking issues preventing Phase 3 work.**
</verification>

<success_criteria>
- handle_media() processes audio through STT + VAD pipeline
- Real-time partial transcripts logged during speech
- Turn detection triggers within 300-700ms of silence
- Final transcripts logged with >70% accuracy on PSTN audio
- Multiple turns handled correctly with VAD reset
- No premature cutoff during natural pauses
- README and E2E testing checklist updated
- Human verification checkpoint approved
</success_criteria>

<output>
After completion, create `.planning/phases/02-speech-to-text-with-streaming/02-03-SUMMARY.md` following the summary template.
</output>
