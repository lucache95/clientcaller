---
phase: 02-speech-to-text-with-streaming
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - requirements.txt
  - src/stt/__init__.py
  - src/stt/processor.py
  - tests/test_stt_processor.py
autonomous: true
requirements: [STT-01]

must_haves:
  truths:
    - "STT processor transcribes audio chunks in real-time"
    - "STT processor streams partial transcripts as user speaks"
    - "STT processor finalizes transcripts when turn completes"
  artifacts:
    - path: "src/stt/processor.py"
      provides: "Streaming STT with faster-whisper backend"
      exports: ["STTProcessor"]
      min_lines: 80
    - path: "tests/test_stt_processor.py"
      provides: "Unit tests for STT transcription"
      min_lines: 40
  key_links:
    - from: "src/stt/processor.py"
      to: "faster-whisper library"
      via: "FasterWhisperASR import"
      pattern: "from whisper_online import FasterWhisperASR"
    - from: "src/stt/processor.py"
      to: "whisper_streaming library"
      via: "OnlineASRProcessor import"
      pattern: "from whisper_online import OnlineASRProcessor"
---

<objective>
Create streaming speech-to-text module using faster-whisper and whisper_streaming for real-time transcription.

Purpose: Enable real-time speech recognition with streaming partial transcripts, achieving sub-200ms transcription latency while maintaining high accuracy on PSTN audio quality.

Output: STTProcessor class that accepts PCM 16kHz audio chunks and yields partial/final transcripts using whisper_streaming's LocalAgreement policy.
</objective>

<execution_context>
@/Users/lucassenechal/.claude/get-shit-done/workflows/execute-plan.md
@/Users/lucassenechal/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-speech-to-text-with-streaming/02-RESEARCH.md

# Phase 1 audio pipeline (provides conversion and resampling)
@.planning/phases/01-telephony-foundation-audio-pipeline/01-01-SUMMARY.md
@.planning/phases/01-telephony-foundation-audio-pipeline/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install faster-whisper and whisper_streaming dependencies</name>
  <files>requirements.txt</files>
  <action>
Add STT dependencies to requirements.txt:

```python
# Speech-to-Text (STT)
faster-whisper>=1.2.0  # CTranslate2-based Whisper for 4x speed improvement
git+https://github.com/ufal/whisper_streaming  # Real-time streaming policy

# PyTorch for model loading (if not already present)
torch>=2.0.0
torchaudio>=2.0.0
```

Install packages:
```bash
pip install faster-whisper>=1.2.0
pip install git+https://github.com/ufal/whisper_streaming
```

**Important:** Do NOT install vanilla `openai-whisper` — we're using `faster-whisper` which is 4x faster via CTranslate2. Per 02-RESEARCH.md, vanilla Whisper has 1-5s latency which blows the 200ms budget.

**Model to use:** `distil-large-v3` (6x faster than large-v3, <1% WER loss). If VRAM is constrained, can fall back to `large-v3` with int8 quantization.
  </action>
  <verify>
```bash
pip list | grep faster-whisper  # Should show >=1.2.0
python -c "from whisper_online import FasterWhisperASR, OnlineASRProcessor; print('whisper_streaming imported')"
python -c "import torch; print('torch imported')"
```
All imports should succeed without errors.
  </verify>
  <done>
faster-whisper and whisper_streaming installed and importable. requirements.txt updated with STT dependencies.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create STTProcessor streaming module</name>
  <files>src/stt/__init__.py, src/stt/processor.py</files>
  <action>
Create `src/stt/processor.py` with streaming transcription using whisper_streaming:

**Architecture (from 02-RESEARCH.md Pattern 1):**
- Load faster-whisper model once at initialization (not per request)
- Use OnlineASRProcessor with LocalAgreement policy for adaptive latency
- Accept PCM 16kHz numpy arrays (already converted by Phase 1 pipeline)
- Yield partial transcripts during speech, final transcript on completion
- Reset state between turns to prevent context bleed

**Key implementation details:**

```python
from whisper_online import FasterWhisperASR, OnlineASRProcessor
import numpy as np
import asyncio

class STTProcessor:
    def __init__(self, model_size="distil-large-v3", language="en"):
        """
        Initialize streaming STT processor.

        Args:
            model_size: "distil-large-v3" (fastest) or "large-v3" (more accurate)
            language: "en" for English-only
        """
        # Load model once at startup (not per call)
        self.asr = FasterWhisperASR(
            language=language,
            modelsize=model_size,
            compute_type="int8"  # Reduces VRAM 11.3GB → 3.1GB
        )
        self.online = OnlineASRProcessor(self.asr)

    async def process_audio_chunk(self, pcm_16khz: np.ndarray):
        """
        Process incoming PCM 16kHz audio chunk.

        Args:
            pcm_16khz: numpy array of int16 audio samples

        Yields:
            dict: {"type": "partial", "text": str} or {"type": "final", "text": str}
        """
        # Convert int16 to float32 normalized to [-1, 1]
        audio_float = pcm_16khz.astype(np.float32) / 32768.0

        # Feed to whisper streaming
        self.online.insert_audio_chunk(audio_float)

        # Get partial transcripts (iterative)
        for timestamp, text in self.online.process_iter():
            yield {"type": "partial", "text": text, "timestamp": timestamp}

    def finalize_turn(self):
        """
        Finalize current turn and get final transcript.

        Returns:
            dict: {"type": "final", "text": str}
        """
        timestamp, final_text = self.online.finish()

        # Reset for next turn (CRITICAL per 02-RESEARCH.md Pitfall 7)
        self.online.init()

        return {"type": "final", "text": final_text, "timestamp": timestamp}
```

Create `src/stt/__init__.py`:
```python
from .processor import STTProcessor

__all__ = ["STTProcessor"]
```

**Why distil-large-v3 over large-v3:** Per 02-RESEARCH.md, 6x faster with <1% WER loss. This is critical for sub-200ms transcription latency (STT-03 requirement).

**Why int8 compute_type:** Reduces VRAM from 11.3GB to 3.1GB, leaving headroom for Gemma 27B and CSM in future phases. Per 02-RESEARCH.md, same accuracy as FP16.

**Why not async wrapper for Whisper:** Per 02-RESEARCH.md Anti-Pattern 5, Whisper inference is synchronous. We'll handle this in Plan 03 integration with `asyncio.to_thread()` to avoid blocking event loop.
  </action>
  <verify>
```bash
python -c "from src.stt import STTProcessor; print('STTProcessor imported')"
python -c "from src.stt.processor import STTProcessor; p = STTProcessor(); print('STTProcessor instantiated')"
```
Both imports should succeed. Model loading may take 10-30 seconds on first run (downloads distil-large-v3).
  </verify>
  <done>
STTProcessor class exists with process_audio_chunk() and finalize_turn() methods. Module exports available via src.stt.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create unit tests for STT processor</name>
  <files>tests/test_stt_processor.py</files>
  <action>
Create `tests/test_stt_processor.py` with basic transcription tests:

**Test strategy:**
- Use short pre-recorded audio clip (e.g., "hello world" in English)
- Verify model loads without errors
- Verify process_audio_chunk() yields partial transcripts
- Verify finalize_turn() returns final transcript
- Verify state reset between turns (no context bleed)

**Note:** Full accuracy testing requires real PSTN audio, which will be done in Plan 03 E2E testing. These unit tests verify the module API and basic functionality.

```python
import pytest
import numpy as np
from src.stt.processor import STTProcessor

@pytest.fixture
def stt_processor():
    """Fixture: STTProcessor instance (loads model once for all tests)"""
    return STTProcessor(model_size="distil-large-v3", language="en")

def test_stt_processor_initialization(stt_processor):
    """Test: STTProcessor loads model without errors"""
    assert stt_processor.asr is not None
    assert stt_processor.online is not None

@pytest.mark.asyncio
async def test_process_audio_chunk_yields_transcripts(stt_processor):
    """Test: process_audio_chunk() yields partial transcripts"""
    # Generate 1 second of silence (16000 samples at 16kHz)
    silence = np.zeros(16000, dtype=np.int16)

    # Process should yield at least empty or partial transcripts
    results = []
    async for result in stt_processor.process_audio_chunk(silence):
        results.append(result)
        if len(results) >= 3:  # Limit iterations
            break

    # Should yield dict with type and text keys
    if results:
        assert "type" in results[0]
        assert "text" in results[0]

def test_finalize_turn_returns_final_transcript(stt_processor):
    """Test: finalize_turn() returns final transcript and resets state"""
    result = stt_processor.finalize_turn()

    assert result["type"] == "final"
    assert "text" in result
    assert isinstance(result["text"], str)
```

**Why silence for testing:** Real audio requires test fixtures or recordings. Silence tests API contract without needing audio files. E2E tests in Plan 03 will validate actual transcription quality.
  </action>
  <verify>
```bash
pytest tests/test_stt_processor.py -v
```
All tests should pass. Model download may take time on first run.
  </verify>
  <done>
Unit tests pass, verifying STTProcessor API contract and basic functionality.
  </done>
</task>

</tasks>

<verification>
All verification criteria:

1. **Dependencies installed:**
   ```bash
   pip list | grep -E "(faster-whisper|torch)"
   python -c "from whisper_online import FasterWhisperASR, OnlineASRProcessor"
   ```

2. **Module structure:**
   ```bash
   ls src/stt/processor.py src/stt/__init__.py
   python -c "from src.stt import STTProcessor"
   ```

3. **Tests pass:**
   ```bash
   pytest tests/test_stt_processor.py -v
   ```

4. **Model loads successfully:**
   ```bash
   python -c "from src.stt import STTProcessor; p = STTProcessor(); print('Model loaded')"
   ```
   Should print "Model loaded" after 10-30 seconds (first run downloads model).

5. **No import errors or blocking issues.**
</verification>

<success_criteria>
- faster-whisper and whisper_streaming installed and importable
- STTProcessor class exists with streaming API (process_audio_chunk, finalize_turn)
- Unit tests pass, verifying basic functionality
- Model loads without errors (distil-large-v3 with int8 quantization)
- Module ready for integration in Plan 03
</success_criteria>

<output>
After completion, create `.planning/phases/02-speech-to-text-with-streaming/02-01-SUMMARY.md` following the summary template.
</output>
