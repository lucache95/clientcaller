---
phase: 02-speech-to-text-with-streaming
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - requirements.txt
  - src/vad/__init__.py
  - src/vad/detector.py
  - tests/test_vad_detector.py
autonomous: true
requirements: [STT-02, STT-03]

must_haves:
  truths:
    - "VAD detects speech/silence boundaries in audio stream"
    - "VAD triggers turn complete within 300ms of user stopping"
    - "VAD handles natural pauses without premature cutoff"
  artifacts:
    - path: "src/vad/detector.py"
      provides: "Voice activity detection with turn detection"
      exports: ["VADDetector"]
      min_lines: 70
    - path: "tests/test_vad_detector.py"
      provides: "Unit tests for VAD functionality"
      min_lines: 30
  key_links:
    - from: "src/vad/detector.py"
      to: "silero-vad library"
      via: "torch.hub.load for Silero VAD model"
      pattern: "torch\\.hub\\.load.*silero-vad"
    - from: "src/vad/detector.py"
      to: "Phase 1 audio pipeline"
      via: "accepts PCM 16kHz numpy arrays"
      pattern: "np\\.ndarray.*int16"
---

<objective>
Create voice activity detection module using Silero VAD for turn detection and interruption handling.

Purpose: Detect when user starts/stops speaking with <300ms latency, enabling natural conversation flow without awkward pauses or premature cutoff.

Output: VADDetector class that processes PCM 16kHz audio chunks and signals speech/silence transitions with configurable thresholds.
</objective>

<execution_context>
@/Users/lucassenechal/.claude/get-shit-done/workflows/execute-plan.md
@/Users/lucassenechal/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-speech-to-text-with-streaming/02-RESEARCH.md

# Phase 1 audio pipeline (provides conversion and resampling)
@.planning/phases/01-telephony-foundation-audio-pipeline/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install Silero VAD dependencies</name>
  <files>requirements.txt</files>
  <action>
Add VAD dependencies to requirements.txt:

```python
# Voice Activity Detection (VAD)
silero-vad>=5.1  # ML-based VAD for speech/silence detection

# PyTorch required for Silero VAD (may already be installed from Plan 01)
torch>=2.0.0
torchaudio>=2.0.0
```

Install packages:
```bash
pip install silero-vad>=5.1
```

**Why Silero VAD over WebRTC VAD:** Per 02-RESEARCH.md, Silero is ML-based with 0.93 F1 score in clean conditions vs WebRTC's GMM-based approach. Handles diverse audio quality better, trained on 6000+ languages, <1ms processing per 30ms chunk on CPU.

**CPU vs GPU for VAD:** Run VAD on CPU (per 02-RESEARCH.md Open Question 3). VAD is lightweight (<1ms/chunk), reserving GPU for Whisper/Gemma/CSM in future phases.
  </action>
  <verify>
```bash
pip list | grep silero-vad  # Should show >=5.1
python -c "import torch; model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=False); print('Silero VAD loaded')"
```
Should print "Silero VAD loaded" (downloads model on first run, ~50-100MB).
  </verify>
  <done>
silero-vad installed and Silero VAD model loads successfully via torch.hub.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create VADDetector module with turn detection</name>
  <files>src/vad/__init__.py, src/vad/detector.py</files>
  <action>
Create `src/vad/detector.py` with Silero VAD integration:

**Architecture (from 02-RESEARCH.md Pattern 4):**
- Load Silero VAD model once at initialization
- Process 30ms audio chunks (480 samples at 16kHz)
- Track speech/silence state with configurable thresholds
- Accumulate silence duration to detect turn completion
- Handle prefix padding (300ms before speech starts)

**Key implementation details:**

```python
import torch
import numpy as np
from typing import Optional, Dict

class VADDetector:
    def __init__(
        self,
        threshold: float = 0.5,
        min_silence_ms: int = 550,
        min_speech_ms: int = 250,
        prefix_padding_ms: int = 300,
        sampling_rate: int = 16000
    ):
        """
        Initialize VAD detector with Silero VAD.

        Args:
            threshold: Speech detection threshold 0-1 (higher = require louder audio)
            min_silence_ms: Silence duration before turn complete (default 550ms per research)
            min_speech_ms: Minimum speech duration to avoid false positives
            prefix_padding_ms: Audio to include before speech starts (avoid clipped words)
            sampling_rate: Must be 8000 or 16000 (Silero VAD requirement)
        """
        # Load Silero VAD via torch.hub (CPU inference)
        self.model, utils = torch.hub.load(
            repo_or_dir='snakers4/silero-vad',
            model='silero_vad',
            force_reload=False
        )

        # Extract utility functions
        (self.get_speech_timestamps, _, _,
         self.vad_iterator, _) = utils

        # Configuration
        self.threshold = threshold
        self.min_silence_ms = min_silence_ms
        self.min_speech_ms = min_speech_ms
        self.prefix_padding_ms = prefix_padding_ms
        self.sampling_rate = sampling_rate

        # State tracking
        self.is_speaking = False
        self.silence_duration_ms = 0
        self.speech_duration_ms = 0

        # Prefix padding buffer (rolling buffer of last 300ms)
        self.prefix_buffer = []
        self.prefix_buffer_max = int(prefix_padding_ms / 20)  # 20ms chunks

    def process_chunk(self, audio_chunk: np.ndarray) -> Dict[str, any]:
        """
        Process audio chunk and detect speech/silence transitions.

        Args:
            audio_chunk: PCM 16kHz int16 numpy array (typically 20-30ms)

        Returns:
            dict: {
                "is_speech": bool,
                "turn_complete": bool,
                "speech_probability": float
            }
        """
        # Convert int16 to float32 normalized to [-1, 1]
        audio_float = audio_chunk.astype(np.float32) / 32768.0

        # Convert to torch tensor
        audio_tensor = torch.from_numpy(audio_float)

        # Run VAD inference (returns speech probability 0-1)
        speech_prob = self.model(audio_tensor, self.sampling_rate).item()

        # Detect speech based on threshold
        is_speech = speech_prob > self.threshold

        # Update prefix buffer (always maintain last 300ms)
        self.prefix_buffer.append(audio_chunk)
        if len(self.prefix_buffer) > self.prefix_buffer_max:
            self.prefix_buffer.pop(0)

        # Track speech/silence durations
        chunk_duration_ms = len(audio_chunk) / self.sampling_rate * 1000

        if is_speech:
            self.speech_duration_ms += chunk_duration_ms
            self.silence_duration_ms = 0  # Reset silence counter

            if not self.is_speaking:
                # Transition: silence → speech
                self.is_speaking = True
        else:
            self.silence_duration_ms += chunk_duration_ms

        # Check for turn completion
        turn_complete = False
        if self.is_speaking and self.silence_duration_ms >= self.min_silence_ms:
            # Sufficient silence after speech → turn complete
            if self.speech_duration_ms >= self.min_speech_ms:
                turn_complete = True

        return {
            "is_speech": is_speech,
            "turn_complete": turn_complete,
            "speech_probability": speech_prob,
            "silence_duration_ms": self.silence_duration_ms,
            "speech_duration_ms": self.speech_duration_ms
        }

    def get_prefix_buffer(self) -> np.ndarray:
        """
        Get prefix padding buffer (audio before speech started).

        Returns:
            np.ndarray: Concatenated audio chunks from prefix buffer
        """
        if not self.prefix_buffer:
            return np.array([], dtype=np.int16)
        return np.concatenate(self.prefix_buffer)

    def reset(self):
        """Reset VAD state for next turn."""
        self.is_speaking = False
        self.silence_duration_ms = 0
        self.speech_duration_ms = 0
        self.prefix_buffer = []
```

Create `src/vad/__init__.py`:
```python
from .detector import VADDetector

__all__ = ["VADDetector"]
```

**Why 550ms min_silence_ms:** Per 02-RESEARCH.md, default 550ms is well-tested for natural conversation. Shorter (300-400ms) risks false positives, longer (700-800ms) feels sluggish. Can be tuned later based on user feedback.

**Why 0.5 threshold:** Per 02-RESEARCH.md, 0.5 is standard for clean audio. Noisy environments may need 0.6-0.7, which Plan 03 can adjust based on testing.

**Why prefix padding:** Per 02-RESEARCH.md Pitfall 2, prevents clipped words at start of speech. VAD detects speech after it starts, so we need to include audio from before trigger.
  </action>
  <verify>
```bash
python -c "from src.vad import VADDetector; print('VADDetector imported')"
python -c "from src.vad.detector import VADDetector; v = VADDetector(); print('VADDetector instantiated')"
```
Both imports should succeed. Model loading may take 10-20 seconds on first run.
  </verify>
  <done>
VADDetector class exists with process_chunk(), get_prefix_buffer(), and reset() methods. Module exports available via src.vad.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create unit tests for VAD detector</name>
  <files>tests/test_vad_detector.py</files>
  <action>
Create `tests/test_vad_detector.py` with basic VAD tests:

**Test strategy:**
- Use synthetic audio (silence, sine wave for "speech")
- Verify VAD initializes without errors
- Verify process_chunk() returns expected dict structure
- Verify turn detection triggers after sufficient silence
- Verify state reset works correctly

```python
import pytest
import numpy as np
from src.vad.detector import VADDetector

@pytest.fixture
def vad_detector():
    """Fixture: VADDetector instance"""
    return VADDetector(
        threshold=0.5,
        min_silence_ms=550,
        min_speech_ms=250
    )

def test_vad_detector_initialization(vad_detector):
    """Test: VADDetector loads model without errors"""
    assert vad_detector.model is not None
    assert vad_detector.threshold == 0.5
    assert vad_detector.min_silence_ms == 550

def test_process_chunk_returns_valid_structure(vad_detector):
    """Test: process_chunk() returns dict with required keys"""
    # Generate 20ms of silence (320 samples at 16kHz)
    silence = np.zeros(320, dtype=np.int16)

    result = vad_detector.process_chunk(silence)

    assert "is_speech" in result
    assert "turn_complete" in result
    assert "speech_probability" in result
    assert isinstance(result["is_speech"], bool)
    assert isinstance(result["turn_complete"], bool)
    assert 0 <= result["speech_probability"] <= 1

def test_turn_detection_triggers_after_silence(vad_detector):
    """Test: Turn complete triggers after min_silence_ms"""
    # Generate speech-like signal (sine wave)
    samples = np.sin(2 * np.pi * 440 * np.arange(320) / 16000) * 10000
    speech = samples.astype(np.int16)

    # Feed speech chunks
    for _ in range(10):  # ~200ms of speech
        vad_detector.process_chunk(speech)

    # Feed silence chunks (should trigger turn complete after ~550ms)
    silence = np.zeros(320, dtype=np.int16)
    results = []
    for _ in range(30):  # ~600ms of silence
        result = vad_detector.process_chunk(silence)
        results.append(result)
        if result["turn_complete"]:
            break

    # Should have triggered turn complete
    turn_complete_count = sum(r["turn_complete"] for r in results)
    assert turn_complete_count > 0, "Turn complete should trigger after silence"

def test_reset_clears_state(vad_detector):
    """Test: reset() clears VAD state"""
    # Generate speech
    speech = (np.sin(2 * np.pi * 440 * np.arange(320) / 16000) * 10000).astype(np.int16)
    vad_detector.process_chunk(speech)

    # Reset state
    vad_detector.reset()

    assert vad_detector.is_speaking == False
    assert vad_detector.silence_duration_ms == 0
    assert vad_detector.speech_duration_ms == 0
```

**Why synthetic audio:** Real PSTN audio testing happens in Plan 03 E2E tests. Unit tests verify API contract and state machine logic.
  </action>
  <verify>
```bash
pytest tests/test_vad_detector.py -v
```
All tests should pass. Model download may take time on first run.
  </verify>
  <done>
Unit tests pass, verifying VADDetector API contract and turn detection logic.
  </done>
</task>

</tasks>

<verification>
All verification criteria:

1. **Dependencies installed:**
   ```bash
   pip list | grep silero-vad
   python -c "import torch; torch.hub.load('snakers4/silero-vad', 'silero_vad')"
   ```

2. **Module structure:**
   ```bash
   ls src/vad/detector.py src/vad/__init__.py
   python -c "from src.vad import VADDetector"
   ```

3. **Tests pass:**
   ```bash
   pytest tests/test_vad_detector.py -v
   ```

4. **Model loads successfully:**
   ```bash
   python -c "from src.vad import VADDetector; v = VADDetector(); print('VAD loaded')"
   ```
   Should print "VAD loaded" after 10-20 seconds.

5. **No import errors or blocking issues.**
</verification>

<success_criteria>
- silero-vad installed and Silero VAD model loads successfully
- VADDetector class exists with turn detection API (process_chunk, reset)
- Unit tests pass, verifying speech/silence detection and turn detection
- Model loads without errors (CPU inference)
- Module ready for integration in Plan 03
</success_criteria>

<output>
After completion, create `.planning/phases/02-speech-to-text-with-streaming/02-02-SUMMARY.md` following the summary template.
</output>
